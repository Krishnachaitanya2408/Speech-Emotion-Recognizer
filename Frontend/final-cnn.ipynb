{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555},{"sourceId":10319115,"sourceType":"datasetVersion","datasetId":6388755},{"sourceId":10365869,"sourceType":"datasetVersion","datasetId":6420379}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:32.417936Z","iopub.execute_input":"2025-01-05T16:52:32.418218Z","iopub.status.idle":"2025-01-05T16:52:32.770763Z","shell.execute_reply.started":"2025-01-05T16:52:32.418196Z","shell.execute_reply":"2025-01-05T16:52:32.769874Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nimport json\n\n# Set random seeds\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:33.277305Z","iopub.execute_input":"2025-01-05T16:52:33.277757Z","iopub.status.idle":"2025-01-05T16:52:36.965796Z","shell.execute_reply.started":"2025-01-05T16:52:33.277729Z","shell.execute_reply":"2025-01-05T16:52:36.965088Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def create_ravdess_df(data_path):\n    file_emotion = []\n    file_path = []\n    valid_emotions = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    \n    for dir in os.listdir(data_path):\n        actor_path = os.path.join(data_path, dir)\n        if os.path.isdir(actor_path):\n            for file in os.listdir(actor_path):\n                part = file.split('.')[0].split('-')\n                emotion = int(part[2])\n                emotion_map = {\n                    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n                    5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'\n                }\n                if emotion_map[emotion] in valid_emotions:\n                    file_emotion.append(emotion_map[emotion])\n                    file_path.append(os.path.join(actor_path, file))\n    \n    return pd.DataFrame({'path': file_path, 'emotion': file_emotion})\n\ndef create_crema_df(data_path):\n    file_emotion = []\n    file_path = []\n    valid_emotions = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    \n    for file in os.listdir(data_path):\n        if file.endswith(\".wav\"):\n            part = file.split('_')\n            emotion_map = {\n                'SAD': 'sad', 'ANG': 'angry', 'DIS': 'disgust',\n                'FEA': 'fear', 'HAP': 'happy', 'NEU': 'neutral'\n            }\n            emotion = emotion_map.get(part[2])\n            if emotion in valid_emotions:\n                file_path.append(os.path.join(data_path, file))\n                file_emotion.append(emotion)\n    \n    return pd.DataFrame({'path': file_path, 'emotion': file_emotion})\n\ndef create_savee_df(data_path):\n    file_emotion = []\n    file_path = []\n    valid_emotions = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    emotion_map = {\n        'a': 'angry', 'd': 'disgust', 'f': 'fear', 'h': 'happy',\n        'n': 'neutral', 'sa': 'sad'\n    }\n    \n    for file in os.listdir(data_path):\n        if file.endswith(\".wav\"):\n            part = file.split('_')[1]\n            ele = part[:-6]\n            emotion = emotion_map.get(ele)\n            if emotion in valid_emotions:\n                file_path.append(os.path.join(data_path, file))\n                file_emotion.append(emotion)\n    \n    return pd.DataFrame({'path': file_path, 'emotion': file_emotion})\n\ndef create_tess_df(data_path):\n    file_emotion = []\n    file_path = []\n    valid_emotions = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    \n    for dir in os.listdir(data_path):\n        dir_path = os.path.join(data_path, dir)\n        if os.path.isdir(dir_path):\n            for file in os.listdir(dir_path):\n                if file.endswith(\".wav\"):\n                    emotion = file.split('.')[0].split('_')[2]\n                    if emotion.lower() in valid_emotions:\n                        file_emotion.append(emotion.lower())\n                        file_path.append(os.path.join(dir_path, file))\n    \n    return pd.DataFrame({'path': file_path, 'emotion': file_emotion})\n\ndef create_telugu_df(data_path):\n    file_emotion = []\n    file_path = []\n    valid_emotions = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    \n    for emotion in os.listdir(data_path):\n        if emotion.lower() in valid_emotions:\n            emotion_path = os.path.join(data_path, emotion)\n            if os.path.isdir(emotion_path):\n                for file in os.listdir(emotion_path):\n                    if file.endswith(\".wav\"):\n                        file_path.append(os.path.join(emotion_path, file))\n                        file_emotion.append(emotion.lower())\n    \n    df = pd.DataFrame({\n        'path': file_path,\n        'emotion': file_emotion\n    })\n    print(f\"Telugu dataset loaded: {len(df)} files\")\n    print(f\"Emotion distribution:\\n{df['emotion'].value_counts()}\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:38.158829Z","iopub.execute_input":"2025-01-05T16:52:38.159266Z","iopub.status.idle":"2025-01-05T16:52:38.171828Z","shell.execute_reply.started":"2025-01-05T16:52:38.159240Z","shell.execute_reply":"2025-01-05T16:52:38.171011Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_all_datasets(ravdess_path, crema_path, savee_path, tess_path, telugu_path):\n    print(\"Loading datasets...\")\n    \n    dfs = []\n    try:\n        ravdess_df = create_ravdess_df(ravdess_path)\n        dfs.append(ravdess_df)\n    except Exception as e:\n        print(f\"Error loading RAVDESS dataset: {str(e)}\")\n    \n    try:\n        crema_df = create_crema_df(crema_path)\n        dfs.append(crema_df)\n    except Exception as e:\n        print(f\"Error loading CREMA dataset: {str(e)}\")\n    \n    try:\n        savee_df = create_savee_df(savee_path)\n        dfs.append(savee_df)\n    except Exception as e:\n        print(f\"Error loading SAVEE dataset: {str(e)}\")\n    \n    try:\n        tess_df = create_tess_df(tess_path)\n        dfs.append(tess_df)\n    except Exception as e:\n        print(f\"Error loading TESS dataset: {str(e)}\")\n    \n    try:\n        telugu_df = create_telugu_df(telugu_path)\n        dfs.append(telugu_df)\n    except Exception as e:\n        print(f\"Error loading Telugu dataset: {str(e)}\")\n    \n    combined_df = pd.concat(dfs, axis=0, ignore_index=True)\n    print(\"\\nDataset Statistics:\")\n    print(f\"Total files: {len(combined_df)}\")\n    print(\"\\nEmotion distribution:\")\n    emotion_counts = combined_df['emotion'].value_counts()\n    print(emotion_counts)\n    \n    return combined_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:39.359131Z","iopub.execute_input":"2025-01-05T16:52:39.359419Z","iopub.status.idle":"2025-01-05T16:52:39.365373Z","shell.execute_reply.started":"2025-01-05T16:52:39.359399Z","shell.execute_reply":"2025-01-05T16:52:39.364578Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def segment_audio(y, sr, segment_length=3, overlap=0.5):\n    \"\"\"\n    Segment audio into fixed-length chunks with overlap\n    segment_length: in seconds\n    overlap: percentage of overlap between segments (0-1)\n    \"\"\"\n    # Calculate segment size in samples\n    segment_samples = int(segment_length * sr)\n    hop_samples = int(segment_samples * (1 - overlap))\n    \n    # Create segments\n    segments = []\n    for i in range(0, len(y) - segment_samples + 1, hop_samples):\n        segment = y[i:i + segment_samples]\n        if len(segment) == segment_samples:  # Only keep complete segments\n            segments.append(segment)\n    \n    # Handle last segment if needed\n    if len(segments) == 0:\n        # If audio is shorter than segment length, pad with zeros\n        segment = np.zeros(segment_samples)\n        segment[:len(y)] = y\n        segments.append(segment)\n        \n    return segments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:40.683604Z","iopub.execute_input":"2025-01-05T16:52:40.683932Z","iopub.status.idle":"2025-01-05T16:52:40.689220Z","shell.execute_reply.started":"2025-01-05T16:52:40.683907Z","shell.execute_reply":"2025-01-05T16:52:40.688296Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def extract_features(file_path, target_sr=44100, segment_length=3, overlap=0.5):\n    try:\n        # Load audio\n        y, sr = librosa.load(file_path, sr=None)\n        if sr != target_sr:\n            y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n        \n        # Segment audio\n        segments = segment_audio(y, target_sr, segment_length, overlap)\n        \n        all_features = []\n        for segment in segments:\n            # Extract features for each segment\n            window_size = int(0.025 * target_sr)\n            hop_length = int(0.010 * target_sr)\n            \n            # Calculate mel spectrogram\n            mel_spec = librosa.feature.melspectrogram(\n                y=segment, sr=target_sr,\n                n_fft=window_size,\n                hop_length=hop_length,\n                n_mels=128\n            )\n            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n            \n            # Calculate additional features for different channels\n            chroma = librosa.feature.chroma_stft(y=segment, sr=target_sr, n_fft=window_size, hop_length=hop_length)\n            spectral_contrast = librosa.feature.spectral_contrast(y=segment, sr=target_sr, n_fft=window_size, hop_length=hop_length)\n            \n            # Normalize each feature independently\n            scaler = StandardScaler()\n            mel_spec_db = scaler.fit_transform(mel_spec_db)\n            chroma = scaler.fit_transform(chroma)\n            spectral_contrast = scaler.fit_transform(spectral_contrast)\n            \n            # Resize additional features to match mel spectrogram dimensions\n            chroma_resized = np.resize(chroma, mel_spec_db.shape)\n            spectral_contrast_resized = np.resize(spectral_contrast, mel_spec_db.shape)\n            \n            # Stack features as RGB channels\n            features = np.stack([\n                mel_spec_db,          # Red channel: Mel spectrogram\n                chroma_resized,       # Green channel: Chromagram\n                spectral_contrast_resized  # Blue channel: Spectral contrast\n            ], axis=0)\n            \n            all_features.append(features)\n        \n        # Average features across segments\n        averaged_features = np.mean(all_features, axis=0) if len(all_features) > 1 else all_features[0]\n        return averaged_features\n\n    except Exception as e:\n        print(f\"Error processing {file_path}: {str(e)}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:42.027069Z","iopub.execute_input":"2025-01-05T16:52:42.027385Z","iopub.status.idle":"2025-01-05T16:52:42.034467Z","shell.execute_reply.started":"2025-01-05T16:52:42.027359Z","shell.execute_reply":"2025-01-05T16:52:42.033572Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class EmotionDataset(Dataset):\n    def __init__(self, df, transform=None, features_cache=None):\n        self.df = df\n        self.transform = transform\n        self.features_cache = features_cache if features_cache is not None else {}\n        self.emotion_to_idx = {\n            emotion: idx for idx, emotion in \n            enumerate(['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust'])\n        }\n        \n        # Print shape of first item in dataset\n        if len(self.features_cache) > 0:\n            first_key = list(self.features_cache.keys())[0]\n            print(f\"\\nDataset feature shape: {self.features_cache[first_key].shape}\")\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        features = self.features_cache[idx]\n        emotion = self.df.iloc[idx]['emotion']\n        emotion_idx = self.emotion_to_idx[emotion]\n        return torch.FloatTensor(features), emotion_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:43.523653Z","iopub.execute_input":"2025-01-05T16:52:43.523994Z","iopub.status.idle":"2025-01-05T16:52:43.529626Z","shell.execute_reply.started":"2025-01-05T16:52:43.523966Z","shell.execute_reply":"2025-01-05T16:52:43.528756Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def extract_and_save_features(df, output_path, target_sr=22050, segment_length=3, overlap=0.5):\n    \"\"\"\n    Extract features from audio files and save them as CSV and dictionary\n    \"\"\"\n    features_cache = {}\n    features_list = []\n    labels = []\n\n    for idx in tqdm(range(len(df))):\n        features = extract_features(df.iloc[idx]['path'], target_sr=target_sr, segment_length=segment_length, overlap=overlap)\n        if features is not None:\n            features_cache[idx] = features\n            features_flat = features.reshape(-1)\n            features_list.append(features_flat)\n            labels.append(df.iloc[idx]['emotion'])\n    \n    # Create feature column names\n    feature_cols = [f'feature_{i}' for i in range(features_list[0].shape[0])]\n    \n    # Create DataFrame\n    features_df = pd.DataFrame(features_list, columns=feature_cols)\n    features_df['emotion'] = labels\n    \n    # Save to CSV\n    print(f\"Features saved to {output_path}\")\n\n    print(f\"Features extracted.\")\n    return features_cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:45.144286Z","iopub.execute_input":"2025-01-05T16:52:45.144652Z","iopub.status.idle":"2025-01-05T16:52:45.150318Z","shell.execute_reply.started":"2025-01-05T16:52:45.144623Z","shell.execute_reply":"2025-01-05T16:52:45.149467Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_data_loaders(train_df, val_df, test_df, train_features_cache, val_features_cache, test_features_cache, batch_size=32):\n    \"\"\"\n    Create DataLoader objects for training, validation, and test sets\n    \"\"\"\n    train_dataset = EmotionDataset(train_df, features_cache = train_features_cache)\n    val_dataset = EmotionDataset(val_df, features_cache = val_features_cache)\n    test_dataset = EmotionDataset(test_df, features_cache = test_features_cache)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:46.858630Z","iopub.execute_input":"2025-01-05T16:52:46.858922Z","iopub.status.idle":"2025-01-05T16:52:46.863709Z","shell.execute_reply.started":"2025-01-05T16:52:46.858901Z","shell.execute_reply":"2025-01-05T16:52:46.862827Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def evaluate_model(model, test_loader, criterion, device, emotion_labels):\n    \"\"\"\n    Evaluate model performance on test set\n    \"\"\"\n    model.eval()\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc='Testing'):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            test_total += labels.size(0)\n            test_correct += (predicted == labels).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    test_loss = test_loss / len(test_loader)\n    test_acc = 100 * test_correct / test_total\n    \n    return test_loss, test_acc, all_preds, all_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:49.113855Z","iopub.execute_input":"2025-01-05T16:52:49.114138Z","iopub.status.idle":"2025-01-05T16:52:49.120148Z","shell.execute_reply.started":"2025-01-05T16:52:49.114117Z","shell.execute_reply":"2025-01-05T16:52:49.119174Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"\n    Plot training and validation metrics\n    \"\"\"\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_acc'], label='Train Accuracy')\n    plt.plot(history['val_acc'], label='Val Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_confusion_matrix(true_labels, pred_labels, class_names):\n    \"\"\"\n    Plot confusion matrix\n    \"\"\"\n    cm = confusion_matrix(true_labels, pred_labels)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n\ndef plot_predicted_vs_actual(true_labels, pred_labels, class_names):\n    \"\"\"\n    Scatter plot of actual labels vs predicted labels.\n    \n    Parameters:\n        true_labels (array-like): True labels.\n        pred_labels (array-like): Predicted labels.\n        class_names (list): List of class names.\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.scatter(range(len(true_labels)), true_labels, label='Actual Labels', marker='o', color='blue', alpha=0.7)\n    plt.scatter(range(len(pred_labels)), pred_labels, label='Predicted Labels', marker='x', color='red', alpha=0.7)\n\n    plt.xlabel('Sample Index')\n    plt.ylabel('Emotion Index')\n    plt.title('Actual vs. Predicted Emotion Labels')\n    plt.yticks(range(len(class_names)), class_names)\n    plt.legend()\n    plt.grid(axis='y')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:51.358895Z","iopub.execute_input":"2025-01-05T16:52:51.359181Z","iopub.status.idle":"2025-01-05T16:52:51.366887Z","shell.execute_reply.started":"2025-01-05T16:52:51.359160Z","shell.execute_reply":"2025-01-05T16:52:51.365981Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class DeepEmotionCNN(nn.Module):\n    def __init__(self, num_classes=6):\n        super(DeepEmotionCNN, self).__init__()\n        \n        # Modified first conv layer to accept 3 channels\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Changed from 1 to 3 channels\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.2)\n        )\n        \n        # Rest of the model remains the same\n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.2)\n        )\n        \n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.3)\n        )\n        \n        self.conv_block4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.4)\n        )\n        \n        self.attention = nn.Sequential(\n            nn.Conv2d(512, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n        \n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        self.fc1 = nn.Linear(512, 1024)\n        self.fc1_bn = nn.BatchNorm1d(1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc2_bn = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc3_bn = nn.BatchNorm1d(256)\n        self.fc4 = nn.Linear(256, num_classes)\n        \n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Forward pass remains the same\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = self.conv_block4(x)\n        \n        attention_weights = self.attention(x)\n        x = x * attention_weights\n        \n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        \n        identity = x\n        x = self.fc1(x)\n        x = self.fc1_bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        x = self.fc2_bn(x)\n        x = self.relu(x + identity)\n        x = self.dropout(x)\n        \n        x = self.fc3(x)\n        x = self.fc3_bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc4(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:53.432330Z","iopub.execute_input":"2025-01-05T16:52:53.432677Z","iopub.status.idle":"2025-01-05T16:52:53.444590Z","shell.execute_reply.started":"2025-01-05T16:52:53.432644Z","shell.execute_reply":"2025-01-05T16:52:53.443585Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    \"\"\"\n    Train the model\n    \"\"\"\n    # Print shape of first batch\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        print(f\"\\nInput shape before feeding to model: {inputs.shape}\")\n        print(f\"Number of classes in batch: {len(torch.unique(labels))}\")\n        break\n        \n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_acc': [],\n        'val_acc': []\n    }\n    \n    best_val_acc = 0\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = train_loss/len(train_loader)\n        train_acc = 100. * correct / total\n\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_acc = 100. * correct / total\n        val_loss = val_loss/len(val_loader)\n        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n\n        scheduler.step(val_acc)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n            }, 'best_model.pth')\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:54.534084Z","iopub.execute_input":"2025-01-05T16:52:54.534386Z","iopub.status.idle":"2025-01-05T16:52:54.543409Z","shell.execute_reply.started":"2025-01-05T16:52:54.534365Z","shell.execute_reply":"2025-01-05T16:52:54.542576Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def load_cached_features(path):\n    features_df = pd.read_csv(path)\n    features_cache = {}\n    \n    # Determine expected feature dimension by looking at the first row\n    first_row = features_df.iloc[0].drop('emotion')\n    feature_size = len(first_row)\n    \n    # Calculate the original shape\n    original_shape = (256, 13)  # From the code we know the original shape is 256x13. \n    \n    if feature_size != np.prod(original_shape):\n      print(f\"Error! The size of the extracted feature is not compatible. Expecting {np.prod(original_shape)} features but found {feature_size}\")\n      return None # Or return empty dict or raise exception if needed\n\n    \n    for idx, row in features_df.iterrows():\n      features = row.drop('emotion').values.astype(float)\n      features = features.reshape(original_shape) # Reshape to original shape here.\n      features_cache[idx] = features\n\n    return features_cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:56.753951Z","iopub.execute_input":"2025-01-05T16:52:56.754240Z","iopub.status.idle":"2025-01-05T16:52:56.759272Z","shell.execute_reply.started":"2025-01-05T16:52:56.754219Z","shell.execute_reply":"2025-01-05T16:52:56.758314Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def main(re_extract_features=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Dataset paths\n    RAVDESS_PATH = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n    CREMA_PATH = \"/kaggle/input/cremad/AudioWAV/\"\n    SAVEE_PATH = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL\"\n    TESS_PATH = \"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data\"\n    TELUGU_PATH = \"/kaggle/input/speech-dataset/drive-download-20241227T163053Z-001\"\n    \n    OUTPUT_PATH = \"output_folder\" # Directory where to save outputs\n    os.makedirs(OUTPUT_PATH, exist_ok=True)\n    \n    # Load datasets\n    combined_df = load_all_datasets(RAVDESS_PATH, CREMA_PATH, SAVEE_PATH, TESS_PATH, TELUGU_PATH)\n    \n    # Split data\n    train_df, temp_df = train_test_split(combined_df, test_size=0.3, stratify=combined_df['emotion'], random_state=42)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['emotion'], random_state=42)\n    \n    # Extract features and cache them, also save them as csv files\n    if re_extract_features: #Re-extract features when `re_extract_features` parameter is True\n        train_features_cache = extract_and_save_features(train_df, os.path.join(OUTPUT_PATH, \"train_features.csv\"))\n        val_features_cache = extract_and_save_features(val_df, os.path.join(OUTPUT_PATH, \"val_features.csv\"))\n        test_features_cache = extract_and_save_features(test_df, os.path.join(OUTPUT_PATH, \"test_features.csv\"))\n    else:\n        train_features_cache = load_cached_features(os.path.join(OUTPUT_PATH, \"train_features.csv\"))\n        val_features_cache = load_cached_features(os.path.join(OUTPUT_PATH, \"val_features.csv\"))\n        test_features_cache = load_cached_features(os.path.join(OUTPUT_PATH, \"test_features.csv\"))\n   \n    if train_features_cache is None or val_features_cache is None or test_features_cache is None:\n      print(\"Error loading cached features. Please re-extract features.\")\n      return\n    \n    # Create datasets and loaders\n    train_loader, val_loader, test_loader = create_data_loaders(\n        train_df, val_df, test_df, train_features_cache, val_features_cache, test_features_cache, batch_size=32\n    )\n    \n   # Initialize model\n    model = DeepEmotionCNN().to(device)\n    \n    #Calculate and print class weights\n    class_counts = combined_df['emotion'].value_counts()\n    total_samples = len(combined_df)\n    \n    # Calculate class weights based on frequencies\n    class_weights = {emotion: total_samples / (len(class_counts) * count) \n                    for emotion, count in class_counts.items()}\n    print(\"\\nClass weights:\")\n    for emotion, weight in class_weights.items():\n        print(f\"{emotion}: {weight:.3f}\")\n        \n    # Sort class weights by label names\n    class_weights_sorted = [class_weights[emotion] for emotion in sorted(class_weights.keys())]\n\n    # Convert class weights to tensor\n    class_weights_tensor = torch.FloatTensor(class_weights_sorted).to(device)\n    \n\n    # Define Loss and Optimizer\n    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Training parameters\n    num_epochs = 50\n    \n    #Train model\n    history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n    \n    #Plot training history\n    plot_training_history(history)\n    \n    # Load best model for evaluation\n    checkpoint = torch.load('best_model.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Evaluate on test set\n    emotions = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    test_loss, test_acc, all_preds, all_labels = evaluate_model(\n        model, test_loader, criterion, device, emotions\n    )\n    print(f'\\nTest accuracy: {test_acc:.2f}%')\n    \n    # Plot confusion matrix\n    plot_confusion_matrix(all_labels, all_preds, emotions)\n\n    # Plot predicted vs. actual labels\n    plot_predicted_vs_actual(all_labels, all_preds, emotions)\n    \n    # Save final results\n    results = {\n        'test_accuracy': test_acc,\n        'confusion_matrix': confusion_matrix(all_labels, all_preds).tolist(),\n        'best_val_accuracy': checkpoint['val_acc']\n    }\n    \n    with open(os.path.join(OUTPUT_PATH, 'results.json'), 'w') as f:\n        json.dump(results, f)\n    \n    # Save final model\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'history': history,\n        'test_accuracy': test_acc\n    }, os.path.join(OUTPUT_PATH, 'final_model.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:52:59.863767Z","iopub.execute_input":"2025-01-05T16:52:59.864094Z","iopub.status.idle":"2025-01-05T16:52:59.874927Z","shell.execute_reply.started":"2025-01-05T16:52:59.864073Z","shell.execute_reply":"2025-01-05T16:52:59.874005Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T16:53:01.473915Z","iopub.execute_input":"2025-01-05T16:53:01.474202Z","execution_failed":"2025-01-05T17:51:22.771Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nTelugu dataset loaded: 168 files\nEmotion distribution:\nemotion\nneutral    42\nsad        31\ndisgust    29\nangry      27\nhappy      23\nfear       16\nName: count, dtype: int64\n\nDataset Statistics:\nTotal files: 11486\n\nEmotion distribution:\nemotion\nsad        1954\ndisgust    1952\nangry      1950\nhappy      1946\nfear       1939\nneutral    1745\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8040/8040 [08:08<00:00, 16.45it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def predict_emotion(audio_path, model_path, device, target_sr=22050):\n    \"\"\"\n    Predict emotion from a given audio file using the trained model.\n\n    Args:\n        audio_path (str): Path to the input audio file.\n        model_path (str): Path to the trained model's checkpoint file (.pth).\n        device (torch.device): Device to use (CPU or GPU).\n        target_sr (int, optional): Target sample rate for audio resampling. Defaults to 22050.\n\n    Returns:\n        dict: Predicted emotion label and emotion probabilities.\n    \"\"\"\n    \n    # Load the trained model\n    checkpoint = torch.load(model_path, map_location=device)\n    model = DeepEmotionCNN().to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()  # Set the model to evaluation mode\n\n    # Extract features from the new audio file\n    features = extract_features(audio_path, target_sr=target_sr)\n    \n    if features is None:\n      print(\"Error in feature extraction. Returning None\")\n      return None\n\n    # Convert to tensor and move to the device\n    features = torch.FloatTensor(features).to(device)\n    \n    # Add a batch dimension (as your model expects batches of data)\n    features = features.unsqueeze(0)  # Now shape [1, 1, height, width]\n\n    # Perform prediction\n    with torch.no_grad():\n        output = model(features)\n        probabilities = torch.softmax(output, dim=1).cpu().numpy() # Get the probabilities of each class\n        _, predicted_idx = torch.max(output, 1)\n        \n    # Convert predicted class index to emotion label\n    emotion_labels = ['happy', 'sad', 'fear', 'angry', 'neutral', 'disgust']\n    predicted_emotion = emotion_labels[predicted_idx.item()]\n\n    return {\"predicted_emotion\": predicted_emotion,\n            \"emotion_probabilities\": dict(zip(emotion_labels, probabilities[0]))}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T06:23:05.146576Z","iopub.execute_input":"2025-01-05T06:23:05.146950Z","iopub.status.idle":"2025-01-05T06:23:05.153879Z","shell.execute_reply.started":"2025-01-05T06:23:05.146924Z","shell.execute_reply":"2025-01-05T06:23:05.152902Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_path = \"/kaggle/working/best_model.pth\"  # Replace with the path to your trained model\naudio_path = \"/kaggle/input/custom-dataset/18balayyapowerfuldialoguesimhamovienbkshor.mp3\" # Replace with the path to the audio you want to classify\n\nprediction_result = predict_emotion(audio_path, model_path, device)\n\nif prediction_result:\n   print(f\"Predicted emotion: {prediction_result['predicted_emotion']}\")\n   print(\"Emotion probabilities:\")\n   for emotion, prob in prediction_result[\"emotion_probabilities\"].items():\n      print(f\"   {emotion}: {prob:.4f}\")\nelse:\n    print(\"Prediction failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T06:23:12.651425Z","iopub.execute_input":"2025-01-05T06:23:12.651744Z","iopub.status.idle":"2025-01-05T06:23:12.998714Z","shell.execute_reply.started":"2025-01-05T06:23:12.651715Z","shell.execute_reply":"2025-01-05T06:23:12.997747Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-927593683728>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"\nFeature extraction output shape: (1, 141, 301)\nPredicted emotion: disgust\nEmotion probabilities:\n   happy: 0.0001\n   sad: 0.0166\n   fear: 0.0004\n   angry: 0.2296\n   neutral: 0.0614\n   disgust: 0.6919\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}